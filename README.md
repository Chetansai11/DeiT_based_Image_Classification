# ðŸ–¼ï¸ Image Classification using Data-efficient Image Transformer (DeiT)

This project demonstrates the implementation and fine-tuning of a **Data-efficient Image Transformer (DeiT-Tiny)** for a custom 7-class image classification task. Using **transfer learning** and **PyTorch**, the model achieves an accuracy of **76%**, exceeding the benchmark of 70%. It highlights the effectiveness of transformer-based architectures in vision tasks, even on moderately sized datasets.

---

## ðŸŽ¯ Project Objective

To build a high-performing image classification model using transformer-based deep learning techniques, particularly **DeiT**, while applying **transfer learning** and **GPU acceleration** for efficiency and scalability.

---

## ðŸ”‘ Key Features

- âš¡ **DeiT Architecture**: Utilized a pre-trained **DeiT-Tiny** model with frozen transformer blocks.
- ðŸ”„ **Transfer Learning**: Adapted to a 7-class classification problem via a custom classifier head.
- ðŸ§ª **Advanced Preprocessing**: Included normalization, rotation, flipping, and resizing to 224Ã—224.
- ðŸš€ **CUDA Acceleration**: Significantly reduced training time using GPU-based computation.
- ðŸ“ˆ **Surpassed Accuracy Target**: Achieved **76% accuracy**, above the 70% goal.

---

## ðŸ—‚ï¸ Dataset Overview

- **Domain**: Architectural classification of 7 types of buildings.
- **Structure**:
  - `Train`: 100% of full dataset (used in training loop)
  - `Validation`: 80% of the training set (via split)
  - `Test`: 20% of total dataset for final evaluation

### ðŸ§¼ Preprocessing Steps
- Resizing to **224x224**
- Data Augmentation:
  - Random rotation
  - Horizontal flipping
  - Pixel normalization

---

## ðŸ§  Model Architecture

| Component             | Description                                                  |
|----------------------|--------------------------------------------------------------|
| ðŸ§© Pre-trained DeiT   | DeiT-Tiny transformer model trained on ImageNet              |
| ðŸ§  Classifier Head    | Fully connected layers + softmax for 7-class classification  |
| â„ï¸ Frozen Layers      | Transformer blocks frozen to retain learned representations  |

ðŸ“˜ Based on the original [DeiT Paper](https://arxiv.org/abs/2012.12877)

---

## âš™ï¸ Training & Optimization

| Parameter        | Value     |
|------------------|-----------|
| Framework        | PyTorch   |
| Optimizer        | AdamW     |
| Loss Function    | Cross-Entropy |
| Learning Rate    | 1e-4      |
| Batch Size       | 128       |
| Epochs           | 200       |

---

## ðŸ“Š Results

| Metric         | Value  |
|----------------|--------|
| **Test Accuracy** | âœ… 76%  |
| **Target Accuracy** | ðŸŽ¯ 70% |
| **Training Speed** | âš¡ Accelerated with CUDA |

![Test Accuracy Result](https://github.com/Chetansai11/DeiT_based_Image_Classification/blob/main/results.png)

The model showed strong **generalization performance** and **low validation loss**, confirming the robustness of preprocessing and architectural choices.

---

## ðŸ” Future Enhancements

- ðŸ” Try larger transformer variants like **DeiT-Small** or **DeiT-Base**
- ðŸ§ª Add AutoAugment, CutMix, or Mixup for improved generalization
- ðŸ“‰ Integrate learning rate schedulers and cyclic LR strategies
- ðŸ§  Explore distillation-based training to compress larger models for deployment

---

## ðŸ› ï¸ Tech Stack

- **Language**: Python  
- **Frameworks**: PyTorch, Torchvision  
- **Techniques**: Transfer Learning, Vision Transformers (ViT), DeiT  
- **Tools**: NumPy, Matplotlib, seaborn, scikit-learn  
- **Hardware**: CUDA-accelerated GPU (NVIDIA)

---

## ðŸ“š References

- [ðŸ“„ DeiT: Training Data-efficient Image Transformers & Distillation through Attention](https://arxiv.org/abs/2012.12877)  
- [ðŸ› ï¸ PyTorch Documentation](https://pytorch.org/)

---

## ðŸ“¬ Contact

**Chetan Sai Borra**  
ðŸ“§ sai311235@gmail.com  
ðŸ”— [LinkedIn](https://www.linkedin.com/in/chetan-sai-16a252251/)

> *This project highlights the power of transformer-based architectures for computer vision, combining pre-trained models and transfer learning for practical, scalable classification tasks.*
